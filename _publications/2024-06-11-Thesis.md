## PhysioLabXR: A Python Platform for Real-Time, Multi-Modal Brainâ€“Computer Interfaces and Extended Reality Experiments


## Overview

This master's thesis encompasses three pioneering projects that explore the intersection of brain-computer interfaces (BCI), human-computer interaction (HCI), and extended reality (XR). The first project, **PhysioLabXR**, introduces an open-source software platform designed for real-time, multi-modal physiological data processing in neuroscience and HCI experiments. The second project, **Interactively Assisting Glaucoma Diagnosis with an Expert Knowledge-Distilled Vision Transformer**, focuses on leveraging deep learning to improve clinical decision-making in glaucoma diagnosis. Lastly, **In Search for an Intuitive and Efficient Text-Entry in Mixed Reality: Tap, Gaze & Pinch, SwEYEpe** investigates new methods for text entry in mixed reality environments, aiming to enhance user interaction through intuitive input techniques.


**Overview**: PhysioLabXR is an open-source software platform designed for neuroscience and human-computer interaction (HCI) experiments. It provides a comprehensive environment for real-time, multi-modal physiological data processing and interactive interfaces&#8203;:contentReference[oaicite:0]{index=0}. Unlike many existing tools, PhysioLabXR supports various data sources, including EEG, EMG, eye trackers, cameras, and fNIRS, and offers advanced features such as multi-stream visualization and digital signal processing (DSP). Its Python-based scripting interface allows researchers to create custom pipelines, making it a versatile tool for developing closed-loop systems and real-time experiments.



**Key Features**:
- **Real-Time Data Processing**: Enables real-time visualization, recording, and replaying of multi-modal data streams, allowing for rapid prototyping and experimentation.
- **Multi-Modal Support**: Native support for electrophysiological sensors, eye trackers, fNIRS, and other data sources, facilitating complex XR and BCI experiments.
- **Extensibility and Customization**: Provides an extensive developer API and a Python scripting interface, allowing users to build and deploy custom data processing pipelines&#8203;:contentReference[oaicite:1]{index=1}.

**Applications**: PhysioLabXR has been successfully applied in various domains, such as virtual reality (VR) and augmented reality (AR) experiments, sensor fusion studies, and neuroscience research. It addresses a critical gap in the research community by offering an all-in-one platform that is both user-friendly and robust, streamlining the development of complex BCI and XR applications.

**Publications**: This work has been published in the *Journal of Open Source Software*&#8203;:contentReference[oaicite:2]{index=2}.

---

## Interactively Assisting Glaucoma Diagnosis with an Expert Knowledge-Distilled Vision Transformer

**Overview**: This project aims to enhance the accuracy and efficiency of glaucoma diagnosis using an expert knowledge-distilled Vision Transformer. By harnessing the power of deep learning and expert knowledge, we developed an AI-based tool that assists ophthalmologists in diagnosing glaucoma. This project is currently submitted to CHI 2025 and presents a novel approach to AI-augmented clinical decision-making.

**Key Features**:
- **Expert Knowledge-Distilled Model**: Integrates domain expertise into the Vision Transformer, allowing the system to focus on critical diagnostic features in retinal images.
- **Augmented Diagnostic Insights**: Offers ophthalmologists augmented insights by highlighting areas of interest in retinal images, thus improving the diagnostic process.
- **User Study**: Conducted with 15 ophthalmologists to validate the system's effectiveness and gather feedback for further improvements&#8203;:contentReference[oaicite:3]{index=3}.

**Significance**: This work represents a step forward in AI-enhanced medical diagnostics, combining deep learning with expert knowledge to provide clinicians with a powerful tool for more accurate and efficient glaucoma diagnosis. It demonstrates how AI can be used to support and enhance clinical decision-making in real-world settings.

**Status**: Submitted to CHI 2025.

---

## In Search for an Intuitive and Efficient Text-Entry in Mixed Reality: Tap, Gaze & Pinch, SwEYEpe

**Overview**: This project explores innovative text-entry methods in mixed reality (MR) environments, aiming to enhance user interaction through intuitive and efficient input techniques. By combining various input modalities like tapping, gaze, pinching, and swiping, this work seeks to overcome the challenges of text entry in MR, such as limited space and the need for natural interactions.

**Key Features**:
- **Multi-Modal Interaction**: Investigates the use of multiple input methods, including gaze, pinch, and swiping, to create a more natural and efficient text-entry experience in MR.
- **User-Centric Design**: Focuses on designing text-entry methods that align with users' natural behaviors and preferences, aiming to improve usability and reduce learning curves.
- **Performance Evaluation**: Includes extensive user studies to evaluate the effectiveness and efficiency of the proposed text-entry methods, providing insights into the optimal design for MR interfaces&#8203;:contentReference[oaicite:4]{index=4}.

**Significance**: This research addresses the pressing need for intuitive and efficient text-entry solutions in MR. By exploring the intersection of various input modalities, it contributes to the development of more user-friendly MR interfaces, enhancing overall user experience in immersive environments.

**Status**: Targeted for CHI 2025 Late-Breaking Work.
